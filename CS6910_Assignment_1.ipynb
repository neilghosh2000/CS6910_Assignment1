{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS6910_Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neilghosh2000/CS6910_Assignment1/blob/main/CS6910_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz5NGtA3mdTa",
        "outputId": "2599ceae-cefc-4239-e1fb-8c7e997b4a18"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.10.21)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.20.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (53.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OylGz6A0Dtb9"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.datasets import fashion_mnist"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0X8wGjVGJDf"
      },
      "source": [
        "#import the data\r\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\n",
        "\r\n",
        "#class names\r\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\r\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "qvIW2rnL5O-C",
        "outputId": "d4c866a0-16c3-42dd-87da-fb0e6475e120"
      },
      "source": [
        "# plot one sample image from each class arnesh\r\n",
        "sample_images = []\r\n",
        "plt.figure(figsize=(10,5))\r\n",
        "for i in range(len(class_names)):\r\n",
        "    sample_images.append(train_labels.tolist().index(i))\r\n",
        "    plt.subplot(2, 5, i+1)\r\n",
        "    plt.xticks([])\r\n",
        "    plt.yticks([])\r\n",
        "    plt.grid(False)\r\n",
        "    plt.imshow(train_images[sample_images[i]], cmap=plt.cm.binary)\r\n",
        "    plt.xlabel(class_names[train_labels[sample_images[i]]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4imZB0mnKMzw"
      },
      "source": [
        "def one_hot_vector(x):\r\n",
        "  y = np.zeros((x.size, x.max()+1))\r\n",
        "  y[np.arange(x.size),x] = 1\r\n",
        "  return y"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TURP1gZDx19"
      },
      "source": [
        "input_size = train_images.shape[1]*train_images.shape[2]\r\n",
        "output_size = len(class_names)\r\n",
        "\r\n",
        "train_x = train_images.reshape([train_images.shape[0], input_size, 1])/255\r\n",
        "test_x = test_images.reshape([test_images.shape[0], input_size, 1])/255\r\n",
        "\r\n",
        "train_y = one_hot_vector(train_labels)\r\n",
        "test_y = one_hot_vector(test_labels)\r\n",
        "\r\n",
        "train_y = np.dstack([train_y])\r\n",
        "test_y = np.dstack([test_y])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_IQUlNd3RiF"
      },
      "source": [
        "nn_layers = [{\"input_dim\" : input_size, \"output_dim\" : 64, \"activation\" : \"sigmoid\"},\r\n",
        "             {\"input_dim\" : 64, \"output_dim\" : 64, \"activation\" : \"sigmoid\"},\r\n",
        "             {\"input_dim\" : 64, \"output_dim\" : 32, \"activation\" : \"sigmoid\"},\r\n",
        "             {\"input_dim\" : 32, \"output_dim\" : output_size, \"activation\" : \"sigmoid\"}]\r\n",
        "             \r\n",
        "def init_layers(nn_layers, seed = 1):\r\n",
        "    np.random.seed(seed)\r\n",
        "    weights = {}\r\n",
        "\r\n",
        "    for i, layer in enumerate(nn_layers):\r\n",
        "        layer_no = i + 1\r\n",
        "        layer_input_size = layer[\"input_dim\"]\r\n",
        "        layer_output_size = layer[\"output_dim\"]\r\n",
        "        \r\n",
        "        weights['w' + str(layer_no)] = np.random.randn(layer_output_size, layer_input_size) * 0.1\r\n",
        "        weights['b' + str(layer_no)] = np.random.randn(layer_output_size, 1) * 0.1\r\n",
        "        \r\n",
        "    return weights\r\n",
        "\r\n",
        "weights = init_layers(nn_layers)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhNlOGf1Pzim"
      },
      "source": [
        "def activation_func(z, activation=\"sigmoid\"):\r\n",
        "    if activation == \"sigmoid\":\r\n",
        "        return 1 / (1 + np.exp(-z))\r\n",
        "    elif activation == \"softmax\":\r\n",
        "        z1 = np.exp(z)\r\n",
        "        z1 = z1.reshape([z1.shape[0], z1.shape[1]])\r\n",
        "        return z1 / np.exp(z).sum(axis=1)\r\n",
        "    elif activation == \"tanh\":\r\n",
        "        return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\r\n",
        "    elif activation == \"relu\":\r\n",
        "        return np.maximum(0, z)\r\n",
        "\r\n",
        "    else:\r\n",
        "        return \"Error\"\r\n",
        "\r\n",
        "def activation_derivative(z, activation=\"sigmoid\"):\r\n",
        "    if activation == \"sigmoid\":\r\n",
        "      sig = 1 / (1 + np.exp(-z))\r\n",
        "      return sig*(1-sig)\r\n",
        "    else:\r\n",
        "      return \"Error\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GFem9XQSzZ3"
      },
      "source": [
        "def forward_prop(train_x, weights, nn_layers):\r\n",
        "    \r\n",
        "    layer_output = {}\r\n",
        "    h_curr = train_x\r\n",
        "    \r\n",
        "    layer_output[\"h0\"] = train_x\r\n",
        "    layer_output[\"a0\"] = train_x\r\n",
        "    \r\n",
        "    for i, layer in enumerate(nn_layers):\r\n",
        "        layer_no = i + 1\r\n",
        "        h_prev = h_curr\r\n",
        "        \r\n",
        "        activation = layer[\"activation\"]\r\n",
        "        w_curr = weights[\"w\" + str(layer_no)]\r\n",
        "        b_curr = weights[\"b\" + str(layer_no)]\r\n",
        "\r\n",
        "        a_curr = np.matmul(w_curr, h_prev) + b_curr\r\n",
        "        h_curr = activation_func(a_curr, activation)\r\n",
        "        \r\n",
        "        layer_output[\"a\" + str(layer_no)] = a_curr\r\n",
        "        layer_output[\"h\" + str(layer_no)] = h_curr\r\n",
        "\r\n",
        "    return h_curr, layer_output"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HichVkAxT6jR"
      },
      "source": [
        "y_hat, layer_output = forward_prop(train_x, weights, nn_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWsjJKH2WDUP"
      },
      "source": [
        "# cross entropy loss\r\n",
        "def loss_func(y_hat, y):\r\n",
        "  return (-np.multiply(y, np.log(y_hat)).sum())/len(y_hat)\r\n",
        "\r\n",
        "\r\n",
        "def accuracy_func(y_hat, train_y):  \r\n",
        "    correct_pred = np.argmax(y_hat, axis = 1) == np.argmax(train_y, axis = 1)\r\n",
        "    return sum(bool(x) for x in correct_pred)/len(y_hat)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv0LV8sA4IQU",
        "outputId": "5b8f82b3-44bf-4303-eec0-1b4ec9ad5b3d"
      },
      "source": [
        "loss_func(y_hat, train_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7433102758911755"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkgqoh355X8-"
      },
      "source": [
        "def back_prop(y_hat, y, layer_output, weights, nn_layers):\r\n",
        "    \r\n",
        "    gradients = {}\r\n",
        "\r\n",
        "    da_prev = -(y - y_hat)\r\n",
        "    da_prev = np.dstack([da_prev])\r\n",
        "\r\n",
        "    m = len(y_hat)\r\n",
        "    \r\n",
        "    for i, layer in reversed(list(enumerate(nn_layers))):\r\n",
        "        layer_index = i + 1\r\n",
        "        activation = layer[\"activation\"]\r\n",
        "        \r\n",
        "        da_curr = da_prev\r\n",
        "                   \r\n",
        "        a_prev = layer_output[\"a\" + str(i)]   \r\n",
        "        h_prev = layer_output[\"h\" + str(i)]\r\n",
        "        dh_prev = np.zeros(h_prev.shape)\r\n",
        "        \r\n",
        "        a_curr = layer_output[\"a\" + str(layer_index)]\r\n",
        "        w_curr = weights[\"w\" + str(layer_index)]\r\n",
        "        b_curr = weights[\"b\" + str(layer_index)]\r\n",
        "        dw_curr = np.zeros(w_curr.shape)\r\n",
        "        db_curr = np.zeros(b_curr.shape)\r\n",
        "\r\n",
        "        for j in range(m):\r\n",
        "            dw_curr += np.dot(da_curr[j], h_prev[j].T) \r\n",
        "            db_curr += da_curr[j]\r\n",
        "            dh_prev[j] = np.dot(w_curr.T, da_curr[j])\r\n",
        "        \r\n",
        "        dw_curr /= m\r\n",
        "        db_curr /= m\r\n",
        " \r\n",
        "        da_prev = np.multiply(dh_prev,activation_derivative(a_prev, activation))\r\n",
        "            \r\n",
        "        gradients[\"dw\" + str(layer_index)] = dw_curr\r\n",
        "        gradients[\"db\" + str(layer_index)] = db_curr\r\n",
        "    \r\n",
        "    return gradients"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDkhAt5187Ia"
      },
      "source": [
        "def gradient_descent(train_x, train_y, test_x, test_y, weights, nn_layers, eta, epochs, n_batches):\r\n",
        "\r\n",
        "  training_loss_list = []\r\n",
        "  training_accuracy_list = []\r\n",
        "  validation_loss_list = []\r\n",
        "  validation_accuracy_list = []\r\n",
        "\r\n",
        "  batch_x = np.array(np.array_split(train_x, n_batches))\r\n",
        "  batch_y = np.array(np.array_split(train_y, n_batches))\r\n",
        "\r\n",
        "  for i in range(epochs):\r\n",
        "    for j in range(n_batches):\r\n",
        "\r\n",
        "        y_hat, layer_output = forward_prop(batch_x[j], weights, nn_layers)\r\n",
        "        gradients = back_prop(y_hat, batch_y[j], layer_output, weights, nn_layers)\r\n",
        "\r\n",
        "        for k, layer in enumerate(nn_layers):\r\n",
        "          weights[\"w\" + str(k+1)] -= eta * gradients[\"dw\" + str(k+1)]        \r\n",
        "          weights[\"b\" + str(k+1)] -= eta * gradients[\"db\" + str(k+1)]\r\n",
        "\r\n",
        "        print(j)\r\n",
        "\r\n",
        "    training_loss, training_accuracy, validation_loss, validation_accuracy = calculate_loss_accuracy(train_x, train_y, test_x, test_y, weights, nn_layers)\r\n",
        "\r\n",
        "    training_loss_list.append(training_loss)\r\n",
        "    training_accuracy_list.append(training_accuracy)\r\n",
        "    validation_loss_list.append(validation_loss)\r\n",
        "    validation_accuracy_list.append(validation_accuracy)\r\n",
        "\r\n",
        "    print((str(i+1)) + \"/\" + str(epochs) + \" epochs completed\")\r\n",
        "\r\n",
        "  return weights, training_loss_list, training_accuracy_list, validation_loss_list, validation_accuracy_list\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbDUJb7TJk42"
      },
      "source": [
        "def calculate_loss_accuracy(train_x, train_y, test_x, test_y, weights, nn_layers):\r\n",
        "\r\n",
        "    y_hat, layer_output = forward_prop(train_x, weights, nn_layers)\r\n",
        "    training_loss = (loss_func(y_hat, train_y))\r\n",
        "    training_accuracy = (accuracy_func(y_hat, train_y))\r\n",
        "\r\n",
        "    y_hat, layer_output = forward_prop(test_x, weights, nn_layers)\r\n",
        "    validation_loss = (loss_func(y_hat, test_y))\r\n",
        "    validation_accuracy = (accuracy_func(y_hat, test_y))\r\n",
        "\r\n",
        "    return training_loss, training_accuracy, validation_loss, validation_accuracy\r\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhHapYC5aeUG"
      },
      "source": [
        "def momentum_gd(train_x, train_y, test_x, test_y, weights, nn_layers, eta, epochs, n_batches):\r\n",
        "\r\n",
        "  training_loss_list = []\r\n",
        "  training_accuracy_list = []\r\n",
        "  validation_loss_list = []\r\n",
        "  validation_accuracy_list = []\r\n",
        "\r\n",
        "  batch_x = np.array(np.array_split(train_x, n_batches))\r\n",
        "  batch_y = np.array(np.array_split(train_y, n_batches))\r\n",
        "\r\n",
        "  prev_weights = {}\r\n",
        "\r\n",
        "  gamma = 0.9\r\n",
        "\r\n",
        "  for k, layer in enumerate(nn_layers):\r\n",
        "          prev_weights[\"w\" + str(k+1)] = np.zeros(weights[\"w\" + str(k+1)].shape)    \r\n",
        "          prev_weights[\"b\" + str(k+1)] = np.zeros(weights[\"b\" + str(k+1)].shape) \r\n",
        "\r\n",
        "  for i in range(epochs):\r\n",
        "    for j in range(n_batches):\r\n",
        "\r\n",
        "        y_hat, layer_output = forward_prop(batch_x[j], weights, nn_layers)\r\n",
        "        gradients = back_prop(y_hat, batch_y[j], layer_output, weights, nn_layers)\r\n",
        "\r\n",
        "        for k, layer in enumerate(nn_layers):\r\n",
        "          prev_weights[\"w\" + str(k+1)] = gamma * prev_weights[\"w\" + str(k+1)] + eta * gradients[\"dw\" + str(k+1)]     \r\n",
        "          prev_weights[\"b\" + str(k+1)] = gamma * prev_weights[\"b\" + str(k+1)] + eta * gradients[\"db\" + str(k+1)]   \r\n",
        "\r\n",
        "          weights[\"w\" + str(k+1)] -= prev_weights[\"w\" + str(k+1)]        \r\n",
        "          weights[\"b\" + str(k+1)] -= prev_weights[\"b\" + str(k+1)]\r\n",
        "\r\n",
        "\r\n",
        "    training_loss, training_accuracy, validation_loss, validation_accuracy = calculate_loss_accuracy(train_x, train_y, test_x, test_y, weights, nn_layers)\r\n",
        "\r\n",
        "    training_loss_list.append(training_loss)\r\n",
        "    training_accuracy_list.append(training_accuracy)\r\n",
        "    validation_loss_list.append(validation_loss)\r\n",
        "    validation_accuracy_list.append(validation_accuracy)\r\n",
        "\r\n",
        "    print((str(i+1)) + \"/\" + str(epochs) + \" completed\")\r\n",
        "\r\n",
        "  return weights, training_loss_list, training_accuracy_list, validation_loss_list, validation_accuracy_list"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh6zj8KEab_a"
      },
      "source": [
        "def nesterov_gd(train_x, train_y, test_x, test_y, weights, nn_layers, eta, epochs, n_batches):\r\n",
        "\r\n",
        "  training_loss_list = []\r\n",
        "  training_accuracy_list = []\r\n",
        "  validation_loss_list = []\r\n",
        "  validation_accuracy_list = []\r\n",
        "\r\n",
        "  batch_x = np.array(np.array_split(train_x, n_batches))\r\n",
        "  batch_y = np.array(np.array_split(train_y, n_batches))\r\n",
        "\r\n",
        "  prev_weights = {}\r\n",
        "  look_ahead_w = {}\r\n",
        "  \r\n",
        "  gamma = 0.9\r\n",
        "\r\n",
        "  for k, layer in enumerate(nn_layers):\r\n",
        "          prev_weights[\"w\" + str(k+1)] = np.zeros(weights[\"w\" + str(k+1)].shape)    \r\n",
        "          prev_weights[\"b\" + str(k+1)] = np.zeros(weights[\"b\" + str(k+1)].shape) \r\n",
        "\r\n",
        "  for i in range(epochs):\r\n",
        "    for j in range(n_batches):\r\n",
        "\r\n",
        "        for k, layer in enumerate(nn_layers):\r\n",
        "          look_ahead_w[\"w\" + str(k+1)] = weights[\"w\" + str(k+1)] - gamma * prev_weights[\"w\" + str(k+1)]\r\n",
        "          look_ahead_w[\"b\" + str(k+1)] = weights[\"b\" + str(k+1)] - gamma * prev_weights[\"b\" + str(k+1)]\r\n",
        "\r\n",
        "        y_hat, layer_output = forward_prop(batch_x[j], look_ahead_w, nn_layers)\r\n",
        "        gradients = back_prop(y_hat, batch_y[j], layer_output, look_ahead_w, nn_layers)\r\n",
        "\r\n",
        "        for k, layer in enumerate(nn_layers):\r\n",
        "          prev_weights[\"w\" + str(k+1)] = gamma * prev_weights[\"w\" + str(k+1)] + eta * gradients[\"dw\" + str(k+1)]     \r\n",
        "          prev_weights[\"b\" + str(k+1)] = gamma * prev_weights[\"b\" + str(k+1)] + eta * gradients[\"db\" + str(k+1)]   \r\n",
        "\r\n",
        "          weights[\"w\" + str(k+1)] -= prev_weights[\"w\" + str(k+1)]        \r\n",
        "          weights[\"b\" + str(k+1)] -= prev_weights[\"b\" + str(k+1)]\r\n",
        "\r\n",
        "\r\n",
        "    training_loss, training_accuracy, validation_loss, validation_accuracy = calculate_loss_accuracy(train_x, train_y, test_x, test_y, weights, nn_layers)\r\n",
        "\r\n",
        "    training_loss_list.append(training_loss)\r\n",
        "    training_accuracy_list.append(training_accuracy)\r\n",
        "    validation_loss_list.append(validation_loss)\r\n",
        "    validation_accuracy_list.append(validation_accuracy)\r\n",
        "\r\n",
        "    print((str(i+1)) + \"/\" + str(epochs) + \" completed\")\r\n",
        "\r\n",
        "  return weights, training_loss_list, training_accuracy_list, validation_loss_list, validation_accuracy_list"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNdJAhPvaaL0"
      },
      "source": [
        "def rmsprop(train_x, train_y, test_x, test_y, weights, nn_layers, eta, epochs, n_batches):\r\n",
        "\r\n",
        "  training_loss_list = []\r\n",
        "  training_accuracy_list = []\r\n",
        "  validation_loss_list = []\r\n",
        "  validation_accuracy_list = []\r\n",
        "\r\n",
        "  batch_x = np.array(np.array_split(train_x, n_batches))\r\n",
        "  batch_y = np.array(np.array_split(train_y, n_batches))\r\n",
        "\r\n",
        "  v = {}\r\n",
        "\r\n",
        "  beta = 0.9\r\n",
        "  epsilon = 1e-8\r\n",
        "\r\n",
        "  for k, layer in enumerate(nn_layers):\r\n",
        "          v[\"w\" + str(k+1)] = np.zeros(weights[\"w\" + str(k+1)].shape)    \r\n",
        "          v[\"b\" + str(k+1)] = np.zeros(weights[\"b\" + str(k+1)].shape) \r\n",
        "\r\n",
        "  for i in range(epochs):\r\n",
        "    for j in range(n_batches):\r\n",
        "\r\n",
        "        y_hat, layer_output = forward_prop(batch_x[j], weights, nn_layers)\r\n",
        "        gradients = back_prop(y_hat, batch_y[j], layer_output, weights, nn_layers)\r\n",
        "\r\n",
        "        for k, layer in enumerate(nn_layers):\r\n",
        "          v[\"w\" + str(k+1)] = beta * v[\"w\" + str(k+1)] + (1-beta) * gradients[\"dw\" + str(k+1)]**2     \r\n",
        "          v[\"b\" + str(k+1)] = beta * v[\"b\" + str(k+1)] + (1-beta) * gradients[\"db\" + str(k+1)]**2   \r\n",
        "\r\n",
        "          weights[\"w\" + str(k+1)] -= eta * np.divide(gradients[\"dw\" + str(k+1)], np.sqrt(v[\"w\" + str(k+1)] + epsilon))        \r\n",
        "          weights[\"b\" + str(k+1)] -= eta * np.divide(gradients[\"db\" + str(k+1)], np.sqrt(v[\"b\" + str(k+1)] + epsilon))\r\n",
        "\r\n",
        "\r\n",
        "    training_loss, training_accuracy, validation_loss, validation_accuracy = calculate_loss_accuracy(train_x, train_y, test_x, test_y, weights, nn_layers)\r\n",
        "\r\n",
        "    training_loss_list.append(training_loss)\r\n",
        "    training_accuracy_list.append(training_accuracy)\r\n",
        "    validation_loss_list.append(validation_loss)\r\n",
        "    validation_accuracy_list.append(validation_accuracy)\r\n",
        "\r\n",
        "    print((str(i+1)) + \"/\" + str(epochs) + \" completed\")\r\n",
        "\r\n",
        "  return weights, training_loss_list, training_accuracy_list, validation_loss_list, validation_accuracy_list"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-knM5XaaYiC"
      },
      "source": [
        "def adam(train_x, train_y, test_x, test_y, weights, nn_layers, eta, epochs, n_batches):\r\n",
        "\r\n",
        "  training_loss_list = []\r\n",
        "  training_accuracy_list = []\r\n",
        "  validation_loss_list = []\r\n",
        "  validation_accuracy_list = []\r\n",
        "\r\n",
        "  batch_x = np.array(np.array_split(train_x, n_batches))\r\n",
        "  batch_y = np.array(np.array_split(train_y, n_batches))\r\n",
        "\r\n",
        "  v = {}\r\n",
        "  v_hat = {}\r\n",
        "  m = {}\r\n",
        "  m_hat = {}\r\n",
        "\r\n",
        "  beta1 = 0.9\r\n",
        "  beta2 = 0.999\r\n",
        "  epsilon = 1e-8\r\n",
        "\r\n",
        "  for k, layer in enumerate(nn_layers):\r\n",
        "          v[\"w\" + str(k+1)] = np.zeros(weights[\"w\" + str(k+1)].shape)    \r\n",
        "          v[\"b\" + str(k+1)] = np.zeros(weights[\"b\" + str(k+1)].shape)\r\n",
        "          m[\"w\" + str(k+1)] = np.zeros(weights[\"w\" + str(k+1)].shape)    \r\n",
        "          m[\"b\" + str(k+1)] = np.zeros(weights[\"b\" + str(k+1)].shape) \r\n",
        "\r\n",
        "  t = 0\r\n",
        "\r\n",
        "  for i in range(epochs):\r\n",
        "    for j in range(n_batches):\r\n",
        "\r\n",
        "        t += 1\r\n",
        "\r\n",
        "        y_hat, layer_output = forward_prop(batch_x[j], weights, nn_layers)\r\n",
        "        gradients = back_prop(y_hat, batch_y[j], layer_output, weights, nn_layers)\r\n",
        "\r\n",
        "        for k, layer in enumerate(nn_layers):\r\n",
        "          v[\"w\" + str(k+1)] = beta2 * v[\"w\" + str(k+1)] + (1-beta2) * gradients[\"dw\" + str(k+1)]**2     \r\n",
        "          v[\"b\" + str(k+1)] = beta2 * v[\"b\" + str(k+1)] + (1-beta2) * gradients[\"db\" + str(k+1)]**2 \r\n",
        "\r\n",
        "          m[\"w\" + str(k+1)] = beta1 * m[\"w\" + str(k+1)] + (1-beta1) * gradients[\"dw\" + str(k+1)]    \r\n",
        "          m[\"b\" + str(k+1)] = beta1 * m[\"b\" + str(k+1)] + (1-beta1) * gradients[\"db\" + str(k+1)] \r\n",
        "\r\n",
        "          v_hat[\"w\" + str(k+1)] = np.divide(v[\"w\" + str(k+1)], (1-beta2**t))\r\n",
        "          v_hat[\"b\" + str(k+1)] = np.divide(v[\"b\" + str(k+1)], (1-beta2**t))\r\n",
        "\r\n",
        "          m_hat[\"w\" + str(k+1)] = np.divide(m[\"w\" + str(k+1)], (1-beta1**t))\r\n",
        "          m_hat[\"b\" + str(k+1)] = np.divide(m[\"b\" + str(k+1)], (1-beta1**t))\r\n",
        "\r\n",
        "          weights[\"w\" + str(k+1)] -= eta * np.divide(m_hat[\"w\" + str(k+1)], np.sqrt(v_hat[\"w\" + str(k+1)] + epsilon))        \r\n",
        "          weights[\"b\" + str(k+1)] -= eta * np.divide(m_hat[\"b\" + str(k+1)], np.sqrt(v_hat[\"b\" + str(k+1)] + epsilon))\r\n",
        "\r\n",
        "\r\n",
        "    training_loss, training_accuracy, validation_loss, validation_accuracy = calculate_loss_accuracy(train_x, train_y, test_x, test_y, weights, nn_layers)\r\n",
        "\r\n",
        "    training_loss_list.append(training_loss)\r\n",
        "    training_accuracy_list.append(training_accuracy)\r\n",
        "    validation_loss_list.append(validation_loss)\r\n",
        "    validation_accuracy_list.append(validation_accuracy)\r\n",
        "\r\n",
        "    print((str(i+1)) + \"/\" + str(epochs) + \" completed\")\r\n",
        "\r\n",
        "  return weights, training_loss_list, training_accuracy_list, validation_loss_list, validation_accuracy_list"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fykglQFQaWpd"
      },
      "source": [
        "def nadam(train_x, train_y, test_x, test_y, weights, nn_layers, eta, epochs, n_batches):\r\n",
        "\r\n",
        "  training_loss_list = []\r\n",
        "  training_accuracy_list = []\r\n",
        "  validation_loss_list = []\r\n",
        "  validation_accuracy_list = []\r\n",
        "\r\n",
        "  batch_x = np.array(np.array_split(train_x, n_batches))\r\n",
        "  batch_y = np.array(np.array_split(train_y, n_batches))\r\n",
        "\r\n",
        "  v = {}\r\n",
        "  v_hat = {}\r\n",
        "  m = {}\r\n",
        "  m_hat = {}\r\n",
        "\r\n",
        "  look_ahead_w = {}\r\n",
        "  look_ahead_m_hat = {}\r\n",
        "  look_ahead_v_hat = {}\r\n",
        "\r\n",
        "  beta1 = 0.9\r\n",
        "  beta2 = 0.999\r\n",
        "  epsilon = 1e-8\r\n",
        "\r\n",
        "  for k, layer in enumerate(nn_layers):\r\n",
        "          v[\"w\" + str(k+1)] = np.zeros(weights[\"w\" + str(k+1)].shape)    \r\n",
        "          v[\"b\" + str(k+1)] = np.zeros(weights[\"b\" + str(k+1)].shape)\r\n",
        "          m[\"w\" + str(k+1)] = np.zeros(weights[\"w\" + str(k+1)].shape)    \r\n",
        "          m[\"b\" + str(k+1)] = np.zeros(weights[\"b\" + str(k+1)].shape) \r\n",
        "\r\n",
        "  t = 0\r\n",
        "\r\n",
        "  for i in range(epochs):\r\n",
        "    for j in range(n_batches):\r\n",
        "\r\n",
        "        t += 1\r\n",
        "\r\n",
        "        for k, layer in enumerate(nn_layers):\r\n",
        "\r\n",
        "          look_ahead_v_hat[\"w\" + str(k+1)] = np.divide(beta2 * v[\"w\" + str(k+1)], (1-beta2**t))\r\n",
        "          look_ahead_v_hat[\"b\" + str(k+1)] = np.divide(beta2 * v[\"b\" + str(k+1)], (1-beta2**t))\r\n",
        "\r\n",
        "          look_ahead_m_hat[\"w\" + str(k+1)] = np.divide(beta1 * m[\"w\" + str(k+1)], (1-beta1**t))\r\n",
        "          look_ahead_m_hat[\"b\" + str(k+1)] = np.divide(beta1 * m[\"b\" + str(k+1)], (1-beta1**t))\r\n",
        "\r\n",
        "          look_ahead_w[\"w\" + str(k+1)] = weights[\"w\" + str(k+1)] - eta * np.divide(look_ahead_m_hat[\"w\" + str(k+1)], np.sqrt(look_ahead_v_hat[\"w\" + str(k+1)] + epsilon))\r\n",
        "          look_ahead_w[\"b\" + str(k+1)] = weights[\"b\" + str(k+1)] - eta * np.divide(look_ahead_m_hat[\"b\" + str(k+1)], np.sqrt(look_ahead_v_hat[\"b\" + str(k+1)] + epsilon))\r\n",
        "\r\n",
        "        y_hat, layer_output = forward_prop(batch_x[j], look_ahead_w, nn_layers)\r\n",
        "        gradients = back_prop(y_hat, batch_y[j], layer_output, look_ahead_w, nn_layers)\r\n",
        "\r\n",
        "        for k, layer in enumerate(nn_layers):\r\n",
        "          \r\n",
        "          v[\"w\" + str(k+1)] = beta2 * v[\"w\" + str(k+1)] + (1-beta2) * gradients[\"dw\" + str(k+1)]**2     \r\n",
        "          v[\"b\" + str(k+1)] = beta2 * v[\"b\" + str(k+1)] + (1-beta2) * gradients[\"db\" + str(k+1)]**2 \r\n",
        "\r\n",
        "          m[\"w\" + str(k+1)] = beta1 * m[\"w\" + str(k+1)] + (1-beta1) * gradients[\"dw\" + str(k+1)]    \r\n",
        "          m[\"b\" + str(k+1)] = beta1 * m[\"b\" + str(k+1)] + (1-beta1) * gradients[\"db\" + str(k+1)] \r\n",
        "\r\n",
        "          v_hat[\"w\" + str(k+1)] = np.divide(v[\"w\" + str(k+1)], (1-beta2**t))\r\n",
        "          v_hat[\"b\" + str(k+1)] = np.divide(v[\"b\" + str(k+1)], (1-beta2**t))\r\n",
        "\r\n",
        "          m_hat[\"w\" + str(k+1)] = np.divide(m[\"w\" + str(k+1)], (1-beta1**t))\r\n",
        "          m_hat[\"b\" + str(k+1)] = np.divide(m[\"b\" + str(k+1)], (1-beta1**t))\r\n",
        "\r\n",
        "          weights[\"w\" + str(k+1)] -= eta * np.divide(m_hat[\"w\" + str(k+1)], np.sqrt(v_hat[\"w\" + str(k+1)] + epsilon))        \r\n",
        "          weights[\"b\" + str(k+1)] -= eta * np.divide(m_hat[\"b\" + str(k+1)], np.sqrt(v_hat[\"b\" + str(k+1)] + epsilon))\r\n",
        "\r\n",
        "\r\n",
        "    training_loss, training_accuracy, validation_loss, validation_accuracy = calculate_loss_accuracy(train_x, train_y, test_x, test_y, weights, nn_layers)\r\n",
        "\r\n",
        "    training_loss_list.append(training_loss)\r\n",
        "    training_accuracy_list.append(training_accuracy)\r\n",
        "    validation_loss_list.append(validation_loss)\r\n",
        "    validation_accuracy_list.append(validation_accuracy)\r\n",
        "\r\n",
        "    print((str(i+1)) + \"/\" + str(epochs) + \" completed\")\r\n",
        "\r\n",
        "  return weights, training_loss_list, training_accuracy_list, validation_loss_list, validation_accuracy_list"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4aDXCVWXLdY"
      },
      "source": [
        "def train(train_x, train_y, test_x, test_y, nn_layers, epochs, eta, batch_size, optimizer):\r\n",
        "    \r\n",
        "    weights = init_layers(nn_layers)\r\n",
        "    \r\n",
        "    n_batches = len(train_x)//batch_size\r\n",
        "\r\n",
        "    if optimizer == \"gradient_descent\":\r\n",
        "      weights, training_loss_list, training_accuracy_list, validation_loss_list, validation_accuracy_list = gradient_descent(train_x, train_y, test_x, test_y, weights, nn_layers, eta, epochs, n_batches)\r\n",
        "\r\n",
        "    elif optimizer == \"momentum_gradient_descent\":\r\n",
        "      weights, training_loss_list, training_accuracy_list, validation_loss_list, validation_accuracy_list = momentum_gd(train_x, train_y, test_x, test_y, weights, nn_layers, eta, epochs, n_batches)\r\n",
        "\r\n",
        "    elif optimizer == \"nesterov_accelerated_gradient_descent\":\r\n",
        "      weights, training_loss_list, training_accuracy_list, validation_loss_list, validation_accuracy_list = nesterov_gd(train_x, train_y, test_x, test_y, weights, nn_layers, eta, epochs, n_batches)\r\n",
        "\r\n",
        "    elif optimizer == \"rmsprop\":\r\n",
        "      weights, training_loss_list, training_accuracy_list, validation_loss_list, validation_accuracy_list = rmsprop(train_x, train_y, test_x, test_y, weights, nn_layers, eta, epochs, n_batches)\r\n",
        "\r\n",
        "    elif optimizer == \"adam\":\r\n",
        "      weights, training_loss_list, training_accuracy_list, validation_loss_list, validation_accuracy_list = adam(train_x, train_y, test_x, test_y, weights, nn_layers, eta, epochs, n_batches)\r\n",
        "\r\n",
        "    elif optimizer == \"nadam\":\r\n",
        "      weights, training_loss_list, training_accuracy_list, validation_loss_list, validation_accuracy_list = nadam(train_x, train_y, test_x, test_y, weights, nn_layers, eta, epochs, n_batches)\r\n",
        "\r\n",
        "    else:\r\n",
        "      return \"Error - Wrong Optimizer\"\r\n",
        "        \r\n",
        "    return weights, training_loss_list, training_accuracy_list, validation_loss_list, validation_accuracy_list"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48Iavyp6wVzs",
        "outputId": "5d97b58c-3ed1-470c-8838-a49945ca2018"
      },
      "source": [
        "eta = 0.001\r\n",
        "epochs = 3\r\n",
        "batch_size = 16\r\n",
        "\r\n",
        "# batch_size = 1 for stochastic and batch_size = len(train_x) for batch updates\r\n",
        "\r\n",
        "optimizer = [\"gradient_descent\", \"momentum_gradient_descent\", \"nesterov_accelerated_gradient_descent\", \"rmsprop\", \"adam\", \"nadam\"]\r\n",
        "\r\n",
        "weights, train_loss, train_accuracy, validation_loss, validation_accuracy = train(train_x, train_y, test_x, test_y, nn_layers, epochs, eta, batch_size, optimizer[1])\r\n",
        "\r\n",
        "# y_hat, layer_output = forward_prop(train_x, weights, nn_layers)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/3 completed\n",
            "2/3 completed\n",
            "3/3 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgZeiRhf3ikV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce7cb34-8a0f-4984-dd93-46420335ad00"
      },
      "source": [
        "train_loss"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.2977850776303743, 2.2732213666201764, 1.7929710069200044]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRrnSowfHVmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2efca3f-9dc0-4419-c0ff-56d9724a0386"
      },
      "source": [
        "train_accuracy"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.11061666666666667, 0.30485, 0.2999]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DATFWQ0wHW8O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}